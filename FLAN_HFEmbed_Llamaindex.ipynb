{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "POC Project: Google FLAN-T5-Large + HuggingFaceEmbedding + LlamaIndexing\n",
        "\n",
        "autratec 2023\n",
        "\n",
        "My overall impression of Google Flan is that it feels like talking to someone at a low primary school level, while GPT4ALL LLM is comparable to a primary 5 or 6 student. ChatGPT, on the other hand, is more like a secondary student.\n",
        "\n",
        "The good news is that running Google Flan won't require too many resources from Google Coblab, although it will still need to be connected to a GPU. However, it cannot be used for commercial or real-life environments. "
      ],
      "metadata": {
        "id": "q7ZYQSZ-S572"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFDDKkJfkQjy"
      },
      "outputs": [],
      "source": [
        "! pip install -q langchain transformers sentence_transformers llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bcRaMpJnI9L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTSimpleVectorIndex, PromptHelper, LLMPredictor, ServiceContext\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.llms.base import LLM\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "aGCX8IBekbNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "FsLK3EreL-Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_size = 512\n",
        "num_output = 200\n",
        "max_chunk_overlap = 20\n",
        "chunk_size_limit = 200"
      ],
      "metadata": {
        "id": "xsJmro6nKDLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customLLM(LLM):\n",
        "    model_name = \"google/flan-t5-large\"\n",
        "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=0, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
        "#    pipeline = pipeline(\"text2text-generation\", model=model_name, device='cpu', model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
        "    def _call(self, prompt, stop=None):\n",
        "        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
        "    def _identifying_params(self):\n",
        "        return {\"name_of_model\": self.model_name}\n",
        "    def _llm_type(self):\n",
        "        return \"custom\"\n",
        "llm_predictor = LLMPredictor(llm=customLLM())"
      ],
      "metadata": {
        "id": "nVcWX65LkfG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple test to ensure llm is working"
      ],
      "metadata": {
        "id": "tAVCJWQWWUpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
	"llm=customLLM()\n",
        "print(llm._call(\"Hi! How is everything going today ?\"))"
      ],
      "metadata": {
        "id": "ENl8l_s_OMHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper, chunk_size_limit = chunk_size_limit) "
      ],
      "metadata": {
        "id": "00Od9sccMD1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pls create a folder under content, call \"data\". Load your txt file for indexing. you should find a file called index.json after completeting the indexing(embedding)"
      ],
      "metadata": {
        "id": "m3ifPVJ6WYWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents,service_context=service_context)\n",
        "index.save_to_disk('index.json')"
      ],
      "metadata": {
        "id": "FSi5cnUcKz15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"How to manage the project chage ?\"\n",
        "response = index.query(query_text,response_mode=\"compact\",service_context=service_context, similarity_top_k=3)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwdOu3K7LNEb",
        "outputId": "fada6768-f7d1-4a45-dc58-f3b4866fbd81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change management is the process whereby changes to work are formally introduced and approved\n"
          ]
        }
      ]
    }
  ]
}