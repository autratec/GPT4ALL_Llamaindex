{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT4ALL+Index: **\n",
        "\n",
        "This is the final version of our working notebook, which has been tested and is now running with GPU support from Colab. Some of the code used in this notebook was referenced from the following source: https://colab.research.google.com/drive/1NWZN15plz8rxrk-9OcxNwwIk1V1MfBsJ?usp=sharing. Instead of using OpenAI's embedding, we utilized HuggingFaceEmbeddings to form the vector\n"
      ],
      "metadata": {
        "id": "rRG1AJGSF-S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install git+https://github.com/huggingface/transformers\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes\n",
        "!pip install llama-index\n",
        "!pip install langchain\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "Crs8TDfaQv6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "KUB19REPzvbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"nomic-ai/gpt4all-lora\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "gpt4all_model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "C932yjAxQ0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Mapping, Any\n",
        "from langchain.llms.base import LLM\n",
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper, LLMPredictor, ServiceContext, GPTSimpleVectorIndex\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding, ServiceContext"
      ],
      "metadata": {
        "id": "GWOvF_T7F2GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_size = 2048\n",
        "num_output = 300\n",
        "max_chunk_overlap = 102\n",
        "chunk_size_limit = 600\n",
        "prompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)"
      ],
      "metadata": {
        "id": "ee9VKVtzF7-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT4ALL_LLM(LLM):\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", )\n",
        "        input_ids = inputs[\"input_ids\"].cuda()\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=0.1,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.2,\n",
        "        )\n",
        "\n",
        "        generation_output = gpt4all_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=num_output,\n",
        "        )\n",
        "        response = tokenizer.decode(generation_output[0],skip_special_tokens=True).strip()\n",
        "        return response[len(prompt):]\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"name_of_model\": \"GPT4ALL\"}\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\""
      ],
      "metadata": {
        "id": "haEvJxl5GIdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please make sure that the chunk_size_limit is set lower than the prompt setting to send only one question at a time to LLM. Too many questions with extensive content in the prompt may confuse GPT4ALL."
      ],
      "metadata": {
        "id": "HmUOiR0elyxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_predictor = LLMPredictor(llm=GPT4ALL_LLM())\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper, chunk_size_limit = 500) "
      ],
      "metadata": {
        "id": "0_nXelUVGN1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please ensure that you create a folder called \"data\" under the Google Colab content directory and load a document in TXT/CSV format for indexing purposes. This will ensure that the index.json file is created with validated information"
      ],
      "metadata": {
        "id": "3xY0iiY5lBij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('./data').load_data()\n",
        "index = GPTSimpleVectorIndex.from_documents(documents,service_context=service_context)\n",
        "index.save_to_disk('index.json')"
      ],
      "metadata": {
        "id": "Vlcty5SAix68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is simple test to ensure LLM model is working. "
      ],
      "metadata": {
        "id": "NIGi246blZvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GPT4ALL_LLM()\n",
        "print(llm._call(\"Hi! How is everythig going ?\"))"
      ],
      "metadata": {
        "id": "n7YMVjQPXWt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please provide your question as query_text related to your document and ensure that K=1 to avoid refined questions. I've noticed that GPT4ALL may get lost in the back-and-forth refinement of questions and answers."
      ],
      "metadata": {
        "id": "JgLaB9JZldmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"how to determine my key project stakeholders ?\"\n",
        "response = index.query(query_text,response_mode=\"compact\",service_context=service_context, similarity_top_k=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "HOUrKsBXGR7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPqGTHiHnOcA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}